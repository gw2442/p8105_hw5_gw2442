---
title: "p8105_hw5_gw2442"
output: github_document
date: "2022-11-10"
---

```{r}
library(tidyverse)
library(rvest)
library(broom)
library(patchwork)
library(dplyr)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2

#### Importing and describing raw data: 

```{r}
homicide_data = read_csv(file = "./data/homicide-data.csv") %>%
  janitor::clean_names()
skimr::skim(homicide_data)
```

The CSV file was downloaded from the GitHub repository, imported, and cleaned. The dataset `homicide_data` has 52,179 rows and 12 columns. The dataset contains information on the homicide ID number, reported date, the victim's first and last name, race, age, sex, location of homicide (including city, state, longtidue, and latitude), and disposition of the case. While there are 0 missing variables for variables `uid`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `disposition`, and `reported date`, there are 60 missing variables for `lat` and `lon`. 


#### Creating variable `city_state`:

```{r}
homicide_data =
  homicide_data %>%
  mutate(
    city_state = as.character(paste(city, state, sep = "_")))
```

#### Creating variable `unsolved_homicide`: 

The variable `unsolved_homicide` was created. Those with the disposition "Closed without arrest" or "Open/No arrest" were categorised as an unsolved homicide and were therefore given a value of 1 in the variable `unsolved_homicide`. Those with the disposition "Closed by arrest" were categorised as a solved homicide and were therefore given a value of 0 in the variable `unsolved_homicide`. 

```{r}
homicide_data = 
  homicide_data %>%
  mutate(unsolved_homicide = ifelse(disposition == "Closed by arrest", 0, 1)) 

homicide_summary_data = 
  homicide_data %>%
  group_by(city_state, unsolved_homicide) %>%
  summarise(n_obs = n())%>%
  spread(key = unsolved_homicide, value = n_obs) %>%
  select(city_state, solved = `0`, unsolved = `1`) %>%
  drop_na()

homicide_summary_data$sum <- homicide_summary_data$solved + homicide_summary_data$unsolved 

homicide_summary_data %>%
  arrange(desc(sum)) %>%
  knitr::kable()
```

The city with the most homicides was Chicago, IL with 5,535 homicides - 1,462 of which were solved and 4,073 were unsolved. Philadelpha, PA is the city with the second most homicides (3,307), followed by Houston, TX (2,842)  

#### `prop.test` for Baltimore 

The function `prop.test` is run for the city of Baltimore to determine the estimate proportion of homicides that are unsovled and its confidence intervals:

```{r}
baltimore_test = 
  homicide_summary_data %>%
  filter(city_state == "Baltimore_MD") 

baltimore_output = 
  prop.test(baltimore_test %>% pull(unsolved), baltimore_test %>% pull(sum), conf.level = 0.95) %>%
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)

baltimore_output %>%
  knitr::kable()
```

#### `prop.test` for each of the cities 

The function `prop.test`is run for each of the cities in the dataset `homicide_summary_dataset`:

```{r}
b = map2(homicide_summary_data$unsolved, homicide_summary_data$sum, ~prop.test(x = .x, n = .y, conf.level = 0.95))
c = map(.x = b, ~broom::tidy(.x))
d = map(.x = c, ~select(.x, estimate, conf.low, conf.high)) %>%
  tibble() %>%
  unnest()

homicide_summary_analysis =
  cbind(homicide_summary_data, d) %>%
  select(city_state, estimate, conf.low, conf.high) %>%
  arrange(desc(estimate)) 
```

The subsequent tidied dataset `homicide_summary_analysis` demonstrates the estimated proportion of unsolved murders, and lower and upper limit of the 95% confidence interval for each city. It is arranged in descending order. This dataset was obtained first using map2 to map the function `prop.test` onto each city. Map was then used to tidy the `prop.test` output, and was used once again to select specifically for the variables `estimate`, `conf.low`, and `conf.high`. 

#### Creating a plot that shows the estimates and confidence intervals for each city:

```{r}
homicide_summary_analysis %>%
  ggplot(aes(x = reorder(city_state, estimate), y = estimate, colour = city_state)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_point() +
  labs(title = "Estimated proportion of unsolved homicides for each city",
       x = "City, State",
       y = "Estimate") +
       theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

The plot above demonstrates the estimated proportion of unsolved homicides for each city, and its corresponding error bars, arranged in ascending order. From the plot, it can be concluded that Chicago, IL has the highest proportion of unsovled murders, followed by New Orleans, LA and Baltimore, MD. 

## Problem 3

#### Setting the design elements for the model `problem_3_mean_p`: 

```{r}
problem_3_mean_p = function(samp_size = 30, mu, sigma = 5) {
  
  sim_data = 
    tibble(
    x = rnorm(n = samp_size, mean = mu, sd = sigma),
  )
  
  sim_data %>% 
    t.test() %>%
    broom::tidy() %>%
    select(estimate, p.value)
    
}
```

#### Repeating simulation for multiple values of mu 

The function created is then applied to mu = 0, 1, 2, 3, 4, 5, and 6. For each value of mu, 5000 datasets are generated. The output is then converted into a tibble with variables `mu`, `estimate`, and `p.value`.

```{r}
n_list = 
  list(
    "mu = 0" = 0,
    "mu = 1" = 1,
    "mu = 2" = 2,
    "mu = 3" = 3,
    "mu = 4" = 4,
    "mu = 5" = 5,
    "mu = 6" = 6
  )

problem_3_output = vector("list", length = 7)

for (i in 1:7) {
  
  problem_3_output[[i]] = 
    rerun(5000, problem_3_mean_p(mu = n_list[[i]])) %>%
    bind_rows()
  
}


problem_3_output = 
tibble(problem_3_output)

mu <- c(0, 1, 2, 3, 4, 5, 6)
problem_3_output ['mu'] <- mu

problem_3_output = 
  unnest(problem_3_output) %>%
  select(mu, estimate, p.value)
```

#### Plot of power vs true value of mu

Creating a plot to demonstrate the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. 

```{r}
problem_3_output %>%
  mutate(reject = ifelse (p.value < 0.05, 1, 0)) %>%
  group_by(mu) %>%
  summarise(power = mean(reject)) %>%
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    title = "Power vs True value of mu ",
    x = "True value of mu",
    y = "Power"
  )
```

Power is the probability that a test of significance will detect a deviation from the null hypothesis, should the deviation exist. The statistical power of a significance test depends on sample size, significance level (alpha), and effect size. As the plot above demonstrates, as effect size increases, the power increases. 

#### Plot of average estimate of mu and true value of mu

#### Plot of average estimate of mu where null was rejected and true value of mu

Creating a plot showing the average estimate of mu on the y axis and true value of mu on the x axis and a plot showing the average estimate of mu only in samples for which the null was rejected on the y axis and the true value of mu on the x axis:

```{r}
problem_3_output %>%
  group_by(mu) %>%
  summarise(mean_mu = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu)) + 
  geom_point() +
  labs(
    title = "Average estimate vs True value of mu",
    x = "True value of mu",
    y = "Average estimate of mu"
  ) +

problem_3_output %>%
  filter(p.value<0.05) %>%
  group_by(mu) %>%
  summarise(mean_mu = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu)) + 
  geom_point() +
  labs(
    title = "Average estimate only for samples where null is rejected vs True value of mu",
    x = "True value of mu",
    y = "Average estimate of mu only for samples where null is rejected")
  
```

As the plots above indicate, the sample average of mu across tests for which the null is rejected is not approximately equal to the true value of mu. 
